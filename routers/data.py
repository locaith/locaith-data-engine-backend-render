from fastapi import APIRouter, HTTPException, Depends, UploadFile, File, Form, Header, Request
from typing import List, Optional
import os
import aiofiles
import tempfile

from models.schemas import DatasetResponse, DatasetCreate, DatasetPreview
from services.lakehouse import lakehouse_service
from services.api_key_service import api_key_service
from services.ai_document_service import ai_doc_intelligence
from routers.auth import get_current_user
from config import settings
from services.auth_service import generate_uuid

router = APIRouter(prefix="/data", tags=["Data Management"])

# API Key Authentication Dependency - reads from middleware or validates header
async def get_api_key_user(request: Request, x_api_key: str = Header(None, alias="X-API-Key")):
    """Validate API Key from header and return user info"""
    # First check if middleware already validated (for performance)
    if hasattr(request.state, 'api_key_info') and request.state.api_key_info:
        return request.state.api_key_info
    
    # Fallback: validate header directly
    if not x_api_key:
        raise HTTPException(
            status_code=401,
            detail="API Key kh√¥ng ƒë∆∞·ª£c cung c·∫•p. Vui l√≤ng th√™m header X-API-Key"
        )
    key_info = api_key_service.validate_key(x_api_key)
    if not key_info:
        raise HTTPException(
            status_code=401,
            detail="API Key kh√¥ng h·ª£p l·ªá ho·∫∑c ƒë√£ h·∫øt h·∫°n"
        )
    return key_info

@router.post("/upload", response_model=DatasetResponse)
async def upload_file(
    file: UploadFile = File(...),
    name: str = Form(...),
    description: Optional[str] = Form(None),
    space_id: Optional[str] = Form(None),
    current_user: dict = Depends(get_current_user)
):
    """Upload a data file (CSV, JSON, Parquet, PDF, Excel, Word, Text, PowerPoint)"""
    # Validate file type - support all common document formats
    allowed_extensions = ['.csv', '.json', '.parquet', '.pdf', '.xlsx', '.xls', '.docx', '.doc', '.txt', '.xml', '.html', '.htm', '.pptx', '.ppt']
    file_ext = os.path.splitext(file.filename)[1].lower()
    
    if file_ext not in allowed_extensions:
        raise HTTPException(
            status_code=400, 
            detail=f"File kh√¥ng h·ªó tr·ª£. Ch·ªâ ch·∫•p nh·∫≠n: {', '.join(allowed_extensions)}"
        )
    
    # Save file temporarily
    temp_path = os.path.join(settings.DATA_DIR, "raw", f"{generate_uuid()}{file_ext}")
    
    async with aiofiles.open(temp_path, 'wb') as f:
        content = await file.read()
        await f.write(content)
    
    try:
        # Ingest file
        result = lakehouse_service.ingest_file(
            file_path=temp_path,
            user_id=current_user["id"],
            name=name,
            description=description,
            space_id=space_id
        )
        
        # Update space file_count if space_id provided
        if space_id:
            from database import get_db
            with get_db() as conn:
                conn.execute("""
                    UPDATE document_spaces 
                    SET file_count = file_count + 1,
                        total_size_mb = total_size_mb + ?,
                        updated_at = CURRENT_TIMESTAMP
                    WHERE id = ? AND user_id = ?
                """, [result["file_size"] / (1024 * 1024), space_id, current_user["id"]])
        
        # Clean up temp file
        os.remove(temp_path)
        
        return DatasetResponse(
            id=result["id"],
            name=result["name"],
            description=description,
            file_type=result["file_type"],
            file_size=result["file_size"],
            row_count=result["row_count"],
            schema_json=str(result["schema"]),
            created_at=result.get("created_at")
        )
    except Exception as e:
        # Clean up on error
        if os.path.exists(temp_path):
            os.remove(temp_path)
        raise HTTPException(status_code=400, detail=str(e))

@router.get("/datasets", response_model=List[DatasetResponse])
async def list_datasets(current_user: dict = Depends(get_current_user)):
    """Get all datasets for current user"""
    datasets = lakehouse_service.get_datasets(current_user["id"])
    return [
        DatasetResponse(
            id=ds["id"],
            name=ds["name"],
            description=ds["description"],
            file_type=ds["file_type"],
            file_size=ds["file_size"],
            row_count=ds["row_count"],
            schema_json=ds["schema_json"],
            created_at=ds["created_at"]
        )
        for ds in datasets
    ]

@router.get("/datasets/{dataset_id}", response_model=DatasetResponse)
async def get_dataset(dataset_id: str, current_user: dict = Depends(get_current_user)):
    """Get a specific dataset"""
    dataset = lakehouse_service.get_dataset(dataset_id, current_user["id"])
    if not dataset:
        raise HTTPException(status_code=404, detail="Dataset kh√¥ng t·ªìn t·∫°i")
    
    return DatasetResponse(
        id=dataset["id"],
        name=dataset["name"],
        description=dataset["description"],
        file_type=dataset["file_type"],
        file_size=dataset["file_size"],
        row_count=dataset["row_count"],
        schema_json=dataset["schema_json"],
        created_at=dataset["created_at"]
    )

@router.get("/datasets/{dataset_id}/preview", response_model=DatasetPreview)
async def preview_dataset(
    dataset_id: str, 
    limit: int = 100,
    current_user: dict = Depends(get_current_user)
):
    """Preview dataset data"""
    result = lakehouse_service.preview_dataset(dataset_id, current_user["id"], limit)
    if not result:
        raise HTTPException(status_code=404, detail="Dataset kh√¥ng t·ªìn t·∫°i")
    
    return DatasetPreview(**result)

@router.delete("/datasets/{dataset_id}")
async def delete_dataset(dataset_id: str, current_user: dict = Depends(get_current_user)):
    """Delete a dataset"""
    success = lakehouse_service.delete_dataset(dataset_id, current_user["id"])
    if not success:
        raise HTTPException(status_code=404, detail="Dataset kh√¥ng t·ªìn t·∫°i")
    
    return {"message": "Dataset ƒë√£ ƒë∆∞·ª£c x√≥a th√†nh c√¥ng"}

# ============ EXTERNAL API (for third-party apps like phechat.com) ============

@router.post("/external/upload")
async def upload_file_external(
    file: UploadFile = File(...),
    name: str = Form(...),
    description: Optional[str] = Form(None),
    api_key_info: dict = Depends(get_api_key_user)
):
    """
    Upload file d√πng API Key thay v√¨ JWT Token.
    D√†nh cho b√™n th·ª© 3 nh∆∞ phechat.com g·ªçi API.
    
    Headers:
        X-API-Key: LcAi_xxx...
    """
    allowed_extensions = ['.csv', '.json', '.parquet', '.pdf', '.xlsx', '.xls', '.docx', '.doc', '.txt', '.xml', '.html', '.htm', '.pptx', '.ppt']
    file_ext = os.path.splitext(file.filename)[1].lower()
    
    if file_ext not in allowed_extensions:
        raise HTTPException(
            status_code=400, 
            detail=f"File kh√¥ng h·ªó tr·ª£. Ch·ªâ ch·∫•p nh·∫≠n: {', '.join(allowed_extensions)}"
        )
    
    api_key_id = api_key_info["key_id"]
    user_id = api_key_info["user_id"]
    
    temp_path = os.path.join(settings.DATA_DIR, "raw", f"{generate_uuid()}{file_ext}")
    
    async with aiofiles.open(temp_path, 'wb') as f:
        content = await file.read()
        await f.write(content)
    
    try:
        # Use API Key isolated ingest (data belongs to API Key, not admin)
        result = lakehouse_service.ingest_file_by_api_key(
            file_path=temp_path,
            api_key_id=api_key_id,
            user_id=user_id,
            name=name,
            description=description
        )
        
        os.remove(temp_path)
        
        return {
            "id": result["id"],
            "name": result["name"],
            "file_type": result["file_type"],
            "file_size": result["file_size"],
            "row_count": result["row_count"],
            "message": "Upload th√†nh c√¥ng"
        }
    except Exception as e:
        if os.path.exists(temp_path):
            os.remove(temp_path)
        raise HTTPException(status_code=400, detail=str(e))

@router.get("/external/datasets")
async def list_datasets_external(api_key_info: dict = Depends(get_api_key_user)):
    """L·∫•y danh s√°ch datasets d√πng API Key (isolated data)"""
    api_key_id = api_key_info["key_id"]
    datasets = lakehouse_service.get_datasets_by_api_key(api_key_id)
    return [
        {
            "id": ds["id"],
            "name": ds["name"],
            "description": ds["description"],
            "file_type": ds["file_type"],
            "file_size": ds["file_size"],
            "row_count": ds["row_count"]
        }
        for ds in datasets
    ]

@router.delete("/external/datasets/{dataset_id}")
async def delete_dataset_external(
    dataset_id: str,
    api_key_info: dict = Depends(get_api_key_user)
):
    """X√≥a dataset d√πng API Key (isolated)"""
    api_key_id = api_key_info["key_id"]
    success = lakehouse_service.delete_dataset_by_api_key(dataset_id, api_key_id)
    if not success:
        raise HTTPException(status_code=404, detail="Dataset kh√¥ng t·ªìn t·∫°i")
    
    return {"message": "Dataset ƒë√£ ƒë∆∞·ª£c x√≥a th√†nh c√¥ng"}

@router.get("/external/datasets/{dataset_id}/preview")
async def preview_dataset_external(
    dataset_id: str,
    limit: int = 100,
    api_key_info: dict = Depends(get_api_key_user)
):
    """
    L·∫•y d·ªØ li·ªáu TH√î t·ª´ dataset - 100% ch√≠nh x√°c, KH√îNG qua AI.
    B√™n th·ª© 3 c√≥ th·ªÉ d√πng AI c·ªßa h·ªç ƒë·ªÉ di·ªÖn gi·∫£i d·ªØ li·ªáu n√†y.
    
    Returns:
        columns: Danh s√°ch t√™n c·ªôt
        data: M·∫£ng c√°c d√≤ng d·ªØ li·ªáu (d·∫°ng object)
        total_rows: T·ªïng s·ªë d√≤ng trong dataset
        preview_rows: S·ªë d√≤ng tr·∫£ v·ªÅ (gi·ªõi h·∫°n b·ªüi limit)
    """
    api_key_id = api_key_info["key_id"]
    result = lakehouse_service.preview_dataset_by_api_key(dataset_id, api_key_id, limit)
    
    if not result:
        raise HTTPException(status_code=404, detail="Dataset kh√¥ng t·ªìn t·∫°i")
    
    if "error" in result:
        raise HTTPException(status_code=400, detail=result["error"])
    
    return result

@router.post("/external/search")
async def search_datasets_external(
    query: str,
    dataset_id: Optional[str] = None,
    limit: int = 50,
    api_key_info: dict = Depends(get_api_key_user)
):
    """
    T√¨m ki·∫øm trong d·ªØ li·ªáu TH√î - 100% ch√≠nh x√°c, KH√îNG qua AI.
    
    Args:
        query: T·ª´ kh√≥a t√¨m ki·∫øm
        dataset_id: (Optional) T√¨m trong dataset c·ª• th·ªÉ
        limit: S·ªë k·∫øt qu·∫£ t·ªëi ƒëa
        
    Returns:
        results: M·∫£ng c√°c d√≤ng ch·ª©a t·ª´ kh√≥a
        total_matches: T·ªïng s·ªë k·∫øt qu·∫£ t√¨m th·∫•y
    """
    api_key_id = api_key_info["key_id"]
    
    # Get datasets
    if dataset_id:
        datasets = [{"id": dataset_id}]
    else:
        datasets = lakehouse_service.get_datasets_by_api_key(api_key_id)
    
    if not datasets:
        raise HTTPException(status_code=400, detail="Ch∆∞a c√≥ dataset n√†o")
    
    all_results = []
    
    for ds in datasets:
        preview = lakehouse_service.preview_dataset_by_api_key(ds["id"], api_key_id, limit=1000)
        if preview and "data" in preview:
            for row in preview["data"]:
                # Search in all columns
                row_str = " ".join(str(v) for v in row.values() if v is not None)
                if query.lower() in row_str.lower():
                    all_results.append({
                        "dataset_id": ds["id"],
                        "dataset_name": ds.get("name", ""),
                        "row": row
                    })
                    if len(all_results) >= limit:
                        break
        
        if len(all_results) >= limit:
            break
    
    return {
        "query": query,
        "results": all_results,
        "total_matches": len(all_results)
    }

@router.get("/external/all-data")
async def get_all_data_external(
    limit_per_dataset: int = 100,
    api_key_info: dict = Depends(get_api_key_user)
):
    """
    L·∫•y T·∫§T C·∫¢ d·ªØ li·ªáu TH√î t·ª´ t·∫•t c·∫£ datasets - ƒë·ªÉ b√™n th·ª© 3 t·ª± x·ª≠ l√Ω.
    
    ƒê√¢y l√† endpoint ch√≠nh ƒë·ªÉ l·∫•y d·ªØ li·ªáu cho AI b√™n th·ª© 3 di·ªÖn gi·∫£i.
    """
    api_key_id = api_key_info["key_id"]
    datasets = lakehouse_service.get_datasets_by_api_key(api_key_id)
    
    if not datasets:
        return {
            "datasets": [],
            "total_datasets": 0,
            "message": "Ch∆∞a c√≥ dataset n√†o. Vui l√≤ng upload file tr∆∞·ªõc."
        }
    
    result = []
    for ds in datasets:
        preview = lakehouse_service.preview_dataset_by_api_key(ds["id"], api_key_id, limit=limit_per_dataset)
        result.append({
            "id": ds["id"],
            "name": ds["name"],
            "file_type": ds["file_type"],
            "row_count": ds["row_count"],
            "columns": preview.get("columns", []) if preview else [],
            "data": preview.get("data", []) if preview else [],
            "preview_rows": preview.get("preview_rows", 0) if preview else 0
        })
    
    return {
        "datasets": result,
        "total_datasets": len(result),
        "note": "D·ªØ li·ªáu th√¥ 100% ch√≠nh x√°c. B√™n th·ª© 3 c√≥ th·ªÉ d√πng AI c·ªßa h·ªç ƒë·ªÉ di·ªÖn gi·∫£i."
    }

@router.post("/external/smart-search")
async def smart_search_external(
    query: str,
    include_provenance: bool = True,
    include_context: bool = True,
    api_key_info: dict = Depends(get_api_key_user)
):
    """
    T√åM KI·∫æM TH√îNG MINH v·ªõi ƒë·∫ßy ƒë·ªß NGU·ªíN G·ªêC (Provenance).
    
    Tr·∫£ v·ªÅ T·∫§T C·∫¢ k·∫øt qu·∫£ kh·ªõp v·ªõi t·ª´ kh√≥a, KH√îNG t√≥m t·∫Øt, KH√îNG b·ªè b·ªõt.
    M·ªói k·∫øt qu·∫£ c√≥ ƒë·∫ßy ƒë·ªß:
    - N·ªôi dung g·ªëc 100%
    - File ngu·ªìn (file n√†o)
    - D√≤ng s·ªë m·∫•y
    - Th·ªùi gian nh·∫≠p
    - G·ª£i √Ω b·ªëi c·∫£nh (n·∫øu c√≥ d·ªØ li·ªáu tr√πng l·∫∑p t·ª´ nhi·ªÅu file)
    
    V√≠ d·ª•: C√¥ng ty X c√≥ 6 file PDF v·ªõi 6 ƒë·ªãa ch·ªâ kh√°c nhau
    ‚Üí Tr·∫£ v·ªÅ T·∫§T C·∫¢ 6 ƒë·ªãa ch·ªâ + ngu·ªìn t·ª´ng file
    """
    api_key_id = api_key_info["key_id"]
    datasets = lakehouse_service.get_datasets_by_api_key(api_key_id)
    
    if not datasets:
        return {
            "query": query,
            "results": [],
            "total_matches": 0,
            "message": "Ch∆∞a c√≥ dataset n√†o. Vui l√≤ng upload file tr∆∞·ªõc."
        }
    
    all_results = []
    files_with_matches = set()
    
    for ds in datasets:
        preview = lakehouse_service.preview_dataset_by_api_key(ds["id"], api_key_id, limit=10000)
        if not preview or "data" not in preview:
            continue
            
        for row in preview["data"]:
            # Search in all columns
            row_str = " ".join(str(v) for v in row.values() if v is not None and not str(v).startswith('_'))
            
            if query.lower() in row_str.lower():
                # Build result with full provenance
                result_item = {
                    "content": {k: v for k, v in row.items() if not k.startswith('_')},
                }
                
                if include_provenance:
                    result_item["provenance"] = {
                        "source_file": row.get("_source_file", ds["name"]),
                        "file_type": row.get("_source_type", ds["file_type"]),
                        "row_number": row.get("_row_number", "unknown"),
                        "dataset_id": ds["id"],
                        "ingested_at": row.get("_ingested_at", "unknown")
                    }
                    files_with_matches.add(row.get("_source_file", ds["name"]))
                
                all_results.append(result_item)
    
    # Add context hints if data found in multiple files
    context_hints = []
    if include_context and len(files_with_matches) > 1:
        context_hints.append({
            "type": "multiple_sources",
            "message": f"T√¨m th·∫•y k·∫øt qu·∫£ t·ª´ {len(files_with_matches)} file kh√°c nhau. ƒê√¢y c√≥ th·ªÉ l√† d·ªØ li·ªáu b·ªï sung ho·∫∑c c√°c phi√™n b·∫£n kh√°c nhau.",
            "files": list(files_with_matches)
        })
    
    if include_context and len(all_results) > 1:
        context_hints.append({
            "type": "multiple_matches",
            "message": f"C√≥ {len(all_results)} k·∫øt qu·∫£ ph√π h·ª£p. T·∫•t c·∫£ ƒë·ªÅu ƒë∆∞·ª£c gi·ªØ nguy√™n, kh√¥ng t√≥m t·∫Øt.",
            "recommendation": "B√™n th·ª© 3 c√≥ th·ªÉ d√πng AI ƒë·ªÉ ph√¢n t√≠ch v√† tr√¨nh b√†y d·ªØ li·ªáu n√†y theo nhu c·∫ßu."
        })
    
    return {
        "query": query,
        "results": all_results,
        "total_matches": len(all_results),
        "total_files_matched": len(files_with_matches),
        "context_hints": context_hints if include_context else [],
        "data_integrity": {
            "is_complete": True,
            "is_summarized": False,
            "is_modified": False,
            "note": "100% d·ªØ li·ªáu g·ªëc v·ªõi ƒë·∫ßy ƒë·ªß ngu·ªìn g·ªëc (provenance)"
        }
    }

@router.post("/external/ai-process")
async def ai_process_file_external(
    file: UploadFile = File(...),
    api_key_info: dict = Depends(get_api_key_user)
):
    """
    üß† AI DOCUMENT INTELLIGENCE - World-Class Processing
    
    S·ª≠ d·ª•ng Gemini-3-Flash ƒë·ªÉ:
    1. Smart OCR - ƒê·ªçc file m·ªù, h·ªèng, scan k√©m
    2. Schema Detection - T·ª± ƒë·ªông nh·∫≠n di·ªán c·∫•u tr√∫c
    3. Entity Extraction - Tr√≠ch xu·∫•t t√™n, ƒë·ªãa ch·ªâ, SƒêT, email...
    4. Table Normalization - Chuy·ªÉn ƒë·ªïi sang b·∫£ng c√≥ c·∫•u tr√∫c
    
    ƒê·∫¢M B·∫¢O:
    - 100% ch√≠nh x√°c - AI ch·ªâ tr√≠ch xu·∫•t, KH√îNG b·ªãa d·ªØ li·ªáu
    - ƒê·∫ßy ƒë·ªß provenance - bi·∫øt d·ªØ li·ªáu t·ª´ ƒë√¢u
    - T·ªëc ƒë·ªô cao - x·ª≠ l√Ω nhanh v·ªõi Gemini-3-Flash
    """
    if not ai_doc_intelligence.is_available():
        raise HTTPException(
            status_code=503,
            detail="AI Document Intelligence ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh. Vui l√≤ng ƒë·∫∑t GEMINI_API_KEY."
        )
    
    # Save uploaded file temporarily
    file_ext = os.path.splitext(file.filename)[1].lower()
    temp_path = os.path.join(settings.DATA_DIR, "temp", f"{generate_uuid()}{file_ext}")
    os.makedirs(os.path.dirname(temp_path), exist_ok=True)
    
    async with aiofiles.open(temp_path, 'wb') as f:
        content = await file.read()
        await f.write(content)
    
    try:
        # Process with AI Document Intelligence
        result = await ai_doc_intelligence.process_document(
            file_path=temp_path,
            file_type=file_ext.lstrip('.'),
            file_name=file.filename
        )
        
        # Cleanup temp file
        if os.path.exists(temp_path):
            os.remove(temp_path)
        
        if not result.get("success"):
            raise HTTPException(
                status_code=400,
                detail=result.get("error", "AI processing failed")
            )
        
        return {
            "success": True,
            "file_name": file.filename,
            "ai_processing": {
                "ocr": result.get("ocr"),
                "schema": result.get("schema"),
                "entities": result.get("entities"),
                "structured_data": result.get("structured_data"),
                "data_quality": result.get("data_quality"),
                "processing_time_ms": result.get("processing_time_ms")
            },
            "provenance": result.get("provenance"),
            "guarantee": {
                "accuracy": "100%",
                "data_invention": False,
                "note": "AI ch·ªâ tr√≠ch xu·∫•t th√¥ng tin c√≥ trong file, KH√îNG b·ªãa th√™m"
            }
        }
        
    except Exception as e:
        if os.path.exists(temp_path):
            os.remove(temp_path)
        raise HTTPException(status_code=500, detail=str(e))
