from fastapi import APIRouter, HTTPException, Depends, UploadFile, File, Form, Header, Request
from typing import List, Optional
import os
import aiofiles
import tempfile

from models.schemas import DatasetResponse, DatasetCreate, DatasetPreview
from services.lakehouse import lakehouse_service
from services.api_key_service import api_key_service
from services.ai_document_service import ai_doc_intelligence
from services.supabase_storage import supabase_storage
from routers.auth import get_current_user
from config import settings
from services.auth_service import generate_uuid

router = APIRouter(prefix="/data", tags=["Data Management"])

# API Key Authentication Dependency - reads from middleware or validates header
async def get_api_key_user(request: Request, x_api_key: str = Header(None, alias="X-API-Key")):
    """Validate API Key from header and return user info"""
    # First check if middleware already validated (for performance)
    if hasattr(request.state, 'api_key_info') and request.state.api_key_info:
        return request.state.api_key_info
    
    # Fallback: validate header directly
    if not x_api_key:
        raise HTTPException(
            status_code=401,
            detail="API Key khÃ´ng Ä‘Æ°á»£c cung cáº¥p. Vui lÃ²ng thÃªm header X-API-Key"
        )
    key_info = api_key_service.validate_key(x_api_key)
    if not key_info:
        raise HTTPException(
            status_code=401,
            detail="API Key khÃ´ng há»£p lá»‡ hoáº·c Ä‘Ã£ háº¿t háº¡n"
        )
    return key_info

@router.post("/upload", response_model=DatasetResponse)
async def upload_file(
    file: UploadFile = File(...),
    name: str = Form(...),
    description: Optional[str] = Form(None),
    space_id: Optional[str] = Form(None),
    current_user: dict = Depends(get_current_user)
):
    """Upload a data file (CSV, JSON, Parquet, PDF, Excel, Word, Text, PowerPoint)"""
    # Validate file type - support all common document formats
    allowed_extensions = ['.csv', '.json', '.parquet', '.pdf', '.xlsx', '.xls', '.docx', '.doc', '.txt', '.xml', '.html', '.htm', '.pptx', '.ppt']
    file_ext = os.path.splitext(file.filename)[1].lower()
    
    if file_ext not in allowed_extensions:
        raise HTTPException(
            status_code=400, 
            detail=f"File khÃ´ng há»— trá»£. Chá»‰ cháº¥p nháº­n: {', '.join(allowed_extensions)}"
        )
    
    # Read file content
    content = await file.read()
    file_id = generate_uuid()
    
    # Try to upload to Supabase Storage first (for persistence)
    storage_url = None
    if supabase_storage.is_available():
        storage_path = f"{current_user['id']}/{file_id}{file_ext}"
        success, result = await supabase_storage.upload_file(content, storage_path)
        if success:
            storage_url = result
            print(f"[Upload] File persisted to Supabase: {storage_url}")
        else:
            print(f"[Upload] Supabase upload failed, using local: {result}")
    
    # Save file locally for processing (temporary)
    temp_path = os.path.join(settings.DATA_DIR, "raw", f"{file_id}{file_ext}")
    os.makedirs(os.path.dirname(temp_path), exist_ok=True)
    
    async with aiofiles.open(temp_path, 'wb') as f:
        await f.write(content)
    
    try:
        # Ingest file (pass storage_url if available)
        result = lakehouse_service.ingest_file(
            file_path=temp_path,
            user_id=current_user["id"],
            name=name,
            description=description,
            space_id=space_id,
            storage_url=storage_url  # Store Supabase URL for later retrieval
        )
        
        # Update space file_count if space_id provided
        if space_id:
            from database import get_db
            with get_db() as conn:
                conn.execute("""
                    UPDATE document_spaces 
                    SET file_count = file_count + 1,
                        total_size_mb = total_size_mb + ?,
                        updated_at = CURRENT_TIMESTAMP
                    WHERE id = ? AND user_id = ?
                """, [result["file_size"] / (1024 * 1024), space_id, current_user["id"]])
        
        # Clean up temp file (we have it in Supabase now)
        if os.path.exists(temp_path):
            os.remove(temp_path)
        
        return DatasetResponse(
            id=result["id"],
            name=result["name"],
            description=description,
            file_type=result["file_type"],
            file_size=result["file_size"],
            row_count=result["row_count"],
            schema_json=str(result["schema"]),
            created_at=result.get("created_at")
        )
    except Exception as e:
        # Clean up on error
        if os.path.exists(temp_path):
            os.remove(temp_path)
        raise HTTPException(status_code=400, detail=str(e))

@router.get("/datasets", response_model=List[DatasetResponse])
async def list_datasets(current_user: dict = Depends(get_current_user)):
    """Get all datasets for current user"""
    datasets = lakehouse_service.get_datasets(current_user["id"])
    return [
        DatasetResponse(
            id=ds["id"],
            name=ds["name"],
            description=ds["description"],
            file_type=ds["file_type"],
            file_size=ds["file_size"],
            row_count=ds["row_count"],
            schema_json=ds["schema_json"],
            created_at=ds["created_at"]
        )
        for ds in datasets
    ]

@router.get("/datasets/{dataset_id}", response_model=DatasetResponse)
async def get_dataset(dataset_id: str, current_user: dict = Depends(get_current_user)):
    """Get a specific dataset"""
    dataset = lakehouse_service.get_dataset(dataset_id, current_user["id"])
    if not dataset:
        raise HTTPException(status_code=404, detail="Dataset khÃ´ng tá»“n táº¡i")
    
    return DatasetResponse(
        id=dataset["id"],
        name=dataset["name"],
        description=dataset["description"],
        file_type=dataset["file_type"],
        file_size=dataset["file_size"],
        row_count=dataset["row_count"],
        schema_json=dataset["schema_json"],
        created_at=dataset["created_at"]
    )

@router.get("/datasets/{dataset_id}/preview", response_model=DatasetPreview)
async def preview_dataset(
    dataset_id: str, 
    limit: int = 100,
    current_user: dict = Depends(get_current_user)
):
    """Preview dataset data"""
    result = lakehouse_service.preview_dataset(dataset_id, current_user["id"], limit)
    if not result:
        raise HTTPException(status_code=404, detail="Dataset khÃ´ng tá»“n táº¡i")
    
    return DatasetPreview(**result)

@router.delete("/datasets/{dataset_id}")
async def delete_dataset(dataset_id: str, current_user: dict = Depends(get_current_user)):
    """Delete a dataset"""
    success = lakehouse_service.delete_dataset(dataset_id, current_user["id"])
    if not success:
        raise HTTPException(status_code=404, detail="Dataset khÃ´ng tá»“n táº¡i")
    
    return {"message": "Dataset Ä‘Ã£ Ä‘Æ°á»£c xÃ³a thÃ nh cÃ´ng"}

# ============ EXTERNAL API (for third-party apps like phechat.com) ============

@router.post("/external/upload")
async def upload_file_external(
    file: UploadFile = File(...),
    name: str = Form(...),
    description: Optional[str] = Form(None),
    api_key_info: dict = Depends(get_api_key_user)
):
    """
    Upload file dÃ¹ng API Key thay vÃ¬ JWT Token.
    DÃ nh cho bÃªn thá»© 3 nhÆ° phechat.com gá»i API.
    
    Headers:
        X-API-Key: LcAi_xxx...
    """
    allowed_extensions = ['.csv', '.json', '.parquet', '.pdf', '.xlsx', '.xls', '.docx', '.doc', '.txt', '.xml', '.html', '.htm', '.pptx', '.ppt']
    file_ext = os.path.splitext(file.filename)[1].lower()
    
    if file_ext not in allowed_extensions:
        raise HTTPException(
            status_code=400, 
            detail=f"File khÃ´ng há»— trá»£. Chá»‰ cháº¥p nháº­n: {', '.join(allowed_extensions)}"
        )
    
    api_key_id = api_key_info["key_id"]
    user_id = api_key_info["user_id"]
    
    temp_path = os.path.join(settings.DATA_DIR, "raw", f"{generate_uuid()}{file_ext}")
    
    async with aiofiles.open(temp_path, 'wb') as f:
        content = await file.read()
        await f.write(content)
    
    try:
        # Use API Key isolated ingest (data belongs to API Key, not admin)
        result = lakehouse_service.ingest_file_by_api_key(
            file_path=temp_path,
            api_key_id=api_key_id,
            user_id=user_id,
            name=name,
            description=description
        )
        
        os.remove(temp_path)
        
        return {
            "id": result["id"],
            "name": result["name"],
            "file_type": result["file_type"],
            "file_size": result["file_size"],
            "row_count": result["row_count"],
            "message": "Upload thÃ nh cÃ´ng"
        }
    except Exception as e:
        if os.path.exists(temp_path):
            os.remove(temp_path)
        raise HTTPException(status_code=400, detail=str(e))

@router.get("/external/datasets")
async def list_datasets_external(api_key_info: dict = Depends(get_api_key_user)):
    """Láº¥y danh sÃ¡ch datasets dÃ¹ng API Key (isolated data)"""
    api_key_id = api_key_info["key_id"]
    datasets = lakehouse_service.get_datasets_by_api_key(api_key_id)
    return [
        {
            "id": ds["id"],
            "name": ds["name"],
            "description": ds["description"],
            "file_type": ds["file_type"],
            "file_size": ds["file_size"],
            "row_count": ds["row_count"]
        }
        for ds in datasets
    ]

@router.delete("/external/datasets/{dataset_id}")
async def delete_dataset_external(
    dataset_id: str,
    api_key_info: dict = Depends(get_api_key_user)
):
    """XÃ³a dataset dÃ¹ng API Key (isolated)"""
    api_key_id = api_key_info["key_id"]
    success = lakehouse_service.delete_dataset_by_api_key(dataset_id, api_key_id)
    if not success:
        raise HTTPException(status_code=404, detail="Dataset khÃ´ng tá»“n táº¡i")
    
    return {"message": "Dataset Ä‘Ã£ Ä‘Æ°á»£c xÃ³a thÃ nh cÃ´ng"}

@router.get("/external/datasets/{dataset_id}/preview")
async def preview_dataset_external(
    dataset_id: str,
    limit: int = 100,
    api_key_info: dict = Depends(get_api_key_user)
):
    """
    Láº¥y dá»¯ liá»‡u THÃ” tá»« dataset - 100% chÃ­nh xÃ¡c, KHÃ”NG qua AI.
    BÃªn thá»© 3 cÃ³ thá»ƒ dÃ¹ng AI cá»§a há» Ä‘á»ƒ diá»…n giáº£i dá»¯ liá»‡u nÃ y.
    
    Returns:
        columns: Danh sÃ¡ch tÃªn cá»™t
        data: Máº£ng cÃ¡c dÃ²ng dá»¯ liá»‡u (dáº¡ng object)
        total_rows: Tá»•ng sá»‘ dÃ²ng trong dataset
        preview_rows: Sá»‘ dÃ²ng tráº£ vá» (giá»›i háº¡n bá»Ÿi limit)
    """
    api_key_id = api_key_info["key_id"]
    result = lakehouse_service.preview_dataset_by_api_key(dataset_id, api_key_id, limit)
    
    if not result:
        raise HTTPException(status_code=404, detail="Dataset khÃ´ng tá»“n táº¡i")
    
    if "error" in result:
        raise HTTPException(status_code=400, detail=result["error"])
    
    return result

@router.post("/external/search")
async def search_datasets_external(
    query: str,
    dataset_id: Optional[str] = None,
    limit: int = 50,
    api_key_info: dict = Depends(get_api_key_user)
):
    """
    TÃ¬m kiáº¿m trong dá»¯ liá»‡u THÃ” - 100% chÃ­nh xÃ¡c, KHÃ”NG qua AI.
    
    Args:
        query: Tá»« khÃ³a tÃ¬m kiáº¿m
        dataset_id: (Optional) TÃ¬m trong dataset cá»¥ thá»ƒ
        limit: Sá»‘ káº¿t quáº£ tá»‘i Ä‘a
        
    Returns:
        results: Máº£ng cÃ¡c dÃ²ng chá»©a tá»« khÃ³a
        total_matches: Tá»•ng sá»‘ káº¿t quáº£ tÃ¬m tháº¥y
    """
    api_key_id = api_key_info["key_id"]
    
    # Get datasets
    if dataset_id:
        datasets = [{"id": dataset_id}]
    else:
        datasets = lakehouse_service.get_datasets_by_api_key(api_key_id)
    
    if not datasets:
        raise HTTPException(status_code=400, detail="ChÆ°a cÃ³ dataset nÃ o")
    
    all_results = []
    
    for ds in datasets:
        preview = lakehouse_service.preview_dataset_by_api_key(ds["id"], api_key_id, limit=1000)
        if preview and "data" in preview:
            for row in preview["data"]:
                # Search in all columns
                row_str = " ".join(str(v) for v in row.values() if v is not None)
                if query.lower() in row_str.lower():
                    all_results.append({
                        "dataset_id": ds["id"],
                        "dataset_name": ds.get("name", ""),
                        "row": row
                    })
                    if len(all_results) >= limit:
                        break
        
        if len(all_results) >= limit:
            break
    
    return {
        "query": query,
        "results": all_results,
        "total_matches": len(all_results)
    }

@router.get("/external/all-data")
async def get_all_data_external(
    limit_per_dataset: int = 100,
    api_key_info: dict = Depends(get_api_key_user)
):
    """
    Láº¥y Táº¤T Cáº¢ dá»¯ liá»‡u THÃ” tá»« táº¥t cáº£ datasets - Ä‘á»ƒ bÃªn thá»© 3 tá»± xá»­ lÃ½.
    
    ÄÃ¢y lÃ  endpoint chÃ­nh Ä‘á»ƒ láº¥y dá»¯ liá»‡u cho AI bÃªn thá»© 3 diá»…n giáº£i.
    """
    api_key_id = api_key_info["key_id"]
    datasets = lakehouse_service.get_datasets_by_api_key(api_key_id)
    
    if not datasets:
        return {
            "datasets": [],
            "total_datasets": 0,
            "message": "ChÆ°a cÃ³ dataset nÃ o. Vui lÃ²ng upload file trÆ°á»›c."
        }
    
    result = []
    for ds in datasets:
        preview = lakehouse_service.preview_dataset_by_api_key(ds["id"], api_key_id, limit=limit_per_dataset)
        result.append({
            "id": ds["id"],
            "name": ds["name"],
            "file_type": ds["file_type"],
            "row_count": ds["row_count"],
            "columns": preview.get("columns", []) if preview else [],
            "data": preview.get("data", []) if preview else [],
            "preview_rows": preview.get("preview_rows", 0) if preview else 0
        })
    
    return {
        "datasets": result,
        "total_datasets": len(result),
        "note": "Dá»¯ liá»‡u thÃ´ 100% chÃ­nh xÃ¡c. BÃªn thá»© 3 cÃ³ thá»ƒ dÃ¹ng AI cá»§a há» Ä‘á»ƒ diá»…n giáº£i."
    }

@router.post("/external/smart-search")
async def smart_search_external(
    query: str,
    include_provenance: bool = True,
    include_context: bool = True,
    api_key_info: dict = Depends(get_api_key_user)
):
    """
    TÃŒM KIáº¾M THÃ”NG MINH vá»›i Ä‘áº§y Ä‘á»§ NGUá»’N Gá»C (Provenance).
    
    Tráº£ vá» Táº¤T Cáº¢ káº¿t quáº£ khá»›p vá»›i tá»« khÃ³a, KHÃ”NG tÃ³m táº¯t, KHÃ”NG bá» bá»›t.
    Má»—i káº¿t quáº£ cÃ³ Ä‘áº§y Ä‘á»§:
    - Ná»™i dung gá»‘c 100%
    - File nguá»“n (file nÃ o)
    - DÃ²ng sá»‘ máº¥y
    - Thá»i gian nháº­p
    - Gá»£i Ã½ bá»‘i cáº£nh (náº¿u cÃ³ dá»¯ liá»‡u trÃ¹ng láº·p tá»« nhiá»u file)
    
    VÃ­ dá»¥: CÃ´ng ty X cÃ³ 6 file PDF vá»›i 6 Ä‘á»‹a chá»‰ khÃ¡c nhau
    â†’ Tráº£ vá» Táº¤T Cáº¢ 6 Ä‘á»‹a chá»‰ + nguá»“n tá»«ng file
    """
    api_key_id = api_key_info["key_id"]
    datasets = lakehouse_service.get_datasets_by_api_key(api_key_id)
    
    if not datasets:
        return {
            "query": query,
            "results": [],
            "total_matches": 0,
            "message": "ChÆ°a cÃ³ dataset nÃ o. Vui lÃ²ng upload file trÆ°á»›c."
        }
    
    all_results = []
    files_with_matches = set()
    
    for ds in datasets:
        preview = lakehouse_service.preview_dataset_by_api_key(ds["id"], api_key_id, limit=10000)
        if not preview or "data" not in preview:
            continue
            
        for row in preview["data"]:
            # Search in all columns
            row_str = " ".join(str(v) for v in row.values() if v is not None and not str(v).startswith('_'))
            
            if query.lower() in row_str.lower():
                # Build result with full provenance
                result_item = {
                    "content": {k: v for k, v in row.items() if not k.startswith('_')},
                }
                
                if include_provenance:
                    result_item["provenance"] = {
                        "source_file": row.get("_source_file", ds["name"]),
                        "file_type": row.get("_source_type", ds["file_type"]),
                        "row_number": row.get("_row_number", "unknown"),
                        "dataset_id": ds["id"],
                        "ingested_at": row.get("_ingested_at", "unknown")
                    }
                    files_with_matches.add(row.get("_source_file", ds["name"]))
                
                all_results.append(result_item)
    
    # Add context hints if data found in multiple files
    context_hints = []
    if include_context and len(files_with_matches) > 1:
        context_hints.append({
            "type": "multiple_sources",
            "message": f"TÃ¬m tháº¥y káº¿t quáº£ tá»« {len(files_with_matches)} file khÃ¡c nhau. ÄÃ¢y cÃ³ thá»ƒ lÃ  dá»¯ liá»‡u bá»• sung hoáº·c cÃ¡c phiÃªn báº£n khÃ¡c nhau.",
            "files": list(files_with_matches)
        })
    
    if include_context and len(all_results) > 1:
        context_hints.append({
            "type": "multiple_matches",
            "message": f"CÃ³ {len(all_results)} káº¿t quáº£ phÃ¹ há»£p. Táº¥t cáº£ Ä‘á»u Ä‘Æ°á»£c giá»¯ nguyÃªn, khÃ´ng tÃ³m táº¯t.",
            "recommendation": "BÃªn thá»© 3 cÃ³ thá»ƒ dÃ¹ng AI Ä‘á»ƒ phÃ¢n tÃ­ch vÃ  trÃ¬nh bÃ y dá»¯ liá»‡u nÃ y theo nhu cáº§u."
        })
    
    return {
        "query": query,
        "results": all_results,
        "total_matches": len(all_results),
        "total_files_matched": len(files_with_matches),
        "context_hints": context_hints if include_context else [],
        "data_integrity": {
            "is_complete": True,
            "is_summarized": False,
            "is_modified": False,
            "note": "100% dá»¯ liá»‡u gá»‘c vá»›i Ä‘áº§y Ä‘á»§ nguá»“n gá»‘c (provenance)"
        }
    }

@router.post("/external/ai-process")
async def ai_process_file_external(
    file: UploadFile = File(...),
    api_key_info: dict = Depends(get_api_key_user)
):
    """
    ðŸ§  AI DOCUMENT INTELLIGENCE - World-Class Processing
    
    Sá»­ dá»¥ng Gemini-3-Flash Ä‘á»ƒ:
    1. Smart OCR - Äá»c file má», há»ng, scan kÃ©m
    2. Schema Detection - Tá»± Ä‘á»™ng nháº­n diá»‡n cáº¥u trÃºc
    3. Entity Extraction - TrÃ­ch xuáº¥t tÃªn, Ä‘á»‹a chá»‰, SÄT, email...
    4. Table Normalization - Chuyá»ƒn Ä‘á»•i sang báº£ng cÃ³ cáº¥u trÃºc
    
    Äáº¢M Báº¢O:
    - 100% chÃ­nh xÃ¡c - AI chá»‰ trÃ­ch xuáº¥t, KHÃ”NG bá»‹a dá»¯ liá»‡u
    - Äáº§y Ä‘á»§ provenance - biáº¿t dá»¯ liá»‡u tá»« Ä‘Ã¢u
    - Tá»‘c Ä‘á»™ cao - xá»­ lÃ½ nhanh vá»›i Gemini-3-Flash
    """
    if not ai_doc_intelligence.is_available():
        raise HTTPException(
            status_code=503,
            detail="AI Document Intelligence chÆ°a Ä‘Æ°á»£c cáº¥u hÃ¬nh. Vui lÃ²ng Ä‘áº·t GEMINI_API_KEY."
        )
    
    # Save uploaded file temporarily
    file_ext = os.path.splitext(file.filename)[1].lower()
    temp_path = os.path.join(settings.DATA_DIR, "temp", f"{generate_uuid()}{file_ext}")
    os.makedirs(os.path.dirname(temp_path), exist_ok=True)
    
    async with aiofiles.open(temp_path, 'wb') as f:
        content = await file.read()
        await f.write(content)
    
    try:
        # Process with AI Document Intelligence
        result = await ai_doc_intelligence.process_document(
            file_path=temp_path,
            file_type=file_ext.lstrip('.'),
            file_name=file.filename
        )
        
        # Cleanup temp file
        if os.path.exists(temp_path):
            os.remove(temp_path)
        
        if not result.get("success"):
            raise HTTPException(
                status_code=400,
                detail=result.get("error", "AI processing failed")
            )
        
        return {
            "success": True,
            "file_name": file.filename,
            "ai_processing": {
                "ocr": result.get("ocr"),
                "schema": result.get("schema"),
                "entities": result.get("entities"),
                "structured_data": result.get("structured_data"),
                "data_quality": result.get("data_quality"),
                "processing_time_ms": result.get("processing_time_ms")
            },
            "provenance": result.get("provenance"),
            "guarantee": {
                "accuracy": "100%",
                "data_invention": False,
                "note": "AI chá»‰ trÃ­ch xuáº¥t thÃ´ng tin cÃ³ trong file, KHÃ”NG bá»‹a thÃªm"
            }
        }
        
    except Exception as e:
        if os.path.exists(temp_path):
            os.remove(temp_path)
        raise HTTPException(status_code=500, detail=str(e))


# ============ GOLD LAYER ENDPOINTS ============

@router.post("/gold/promote/{dataset_id}")
async def promote_dataset_to_gold(
    dataset_id: str,
    force: bool = False,
    current_user: dict = Depends(get_current_user)
):
    """
    Promote a dataset to Gold Layer for 100% accurate SQL queries
    
    - Extracts structured tables from file
    - Validates and cleans data
    - Stores in queryable Gold tables
    """
    from services.gold_layer_service import gold_layer_service
    from database import get_db
    
    # Verify ownership and get file path
    with get_db() as conn:
        dataset = conn.execute("""
            SELECT id, file_path, space_id, name
            FROM datasets
            WHERE id = ? AND user_id = ?
        """, [dataset_id, current_user["id"]]).fetchone()
        
        if not dataset:
            raise HTTPException(status_code=404, detail="Dataset not found")
        
        file_path = dataset[1]
        space_id = dataset[2]
    
    # Promote to Gold
    result = await gold_layer_service.promote_to_gold(
        dataset_id=dataset_id,
        file_path=file_path,
        space_id=space_id,
        force=force
    )
    
    return result


@router.get("/gold/tables/{space_id}")
async def get_gold_tables(
    space_id: str,
    current_user: dict = Depends(get_current_user)
):
    """Get all Gold tables available for SQL queries in a space"""
    from services.gold_layer_service import gold_layer_service
    from database import get_db
    
    # Verify space ownership
    with get_db() as conn:
        space = conn.execute("""
            SELECT id FROM document_spaces WHERE id = ? AND user_id = ?
        """, [space_id, current_user["id"]]).fetchone()
        
        if not space:
            raise HTTPException(status_code=404, detail="Space not found")
    
    tables = await gold_layer_service.get_gold_tables(space_id)
    return {
        "space_id": space_id,
        "gold_tables": tables,
        "total": len(tables),
        "queryable": True
    }


@router.post("/gold/query/{space_id}")
async def query_gold_tables(
    space_id: str,
    sql: str = Form(...),
    limit: int = Form(1000),
    current_user: dict = Depends(get_current_user)
):
    """
    Execute SQL query on Gold tables - 100% accuracy
    
    Example: SELECT * FROM hardware WHERE so_luong > 5
    """
    from services.gold_layer_service import gold_layer_service
    from database import get_db
    
    # Verify space ownership
    with get_db() as conn:
        space = conn.execute("""
            SELECT id FROM document_spaces WHERE id = ? AND user_id = ?
        """, [space_id, current_user["id"]]).fetchone()
        
        if not space:
            raise HTTPException(status_code=404, detail="Space not found")
    
    result = await gold_layer_service.query_gold(space_id, sql, limit)
    return result


@router.get("/gold/preview/{gold_table_id}")
async def preview_gold_table(
    gold_table_id: str,
    limit: int = 100,
    current_user: dict = Depends(get_current_user)
):
    """Preview data from a Gold table"""
    from services.gold_layer_service import gold_layer_service
    
    result = await gold_layer_service.get_table_preview(gold_table_id, limit)
    return result

